import { spawn } from "child_process";
import { DynamoDBClient, CreateTableCommand, DeleteTableCommand, ListTablesCommand } from "@aws-sdk/client-dynamodb";
import { DynamoDBDocumentClient, ScanCommand, PutCommand } from "@aws-sdk/lib-dynamodb";

const PRODUCTION_REGION = process.env.AWS_REGION || "eu-west-2";
const LOCAL_ENDPOINT = "http://localhost:8000";

export async function setupLocalDB(): Promise<void> {
  try {
    console.log("üóëÔ∏è  Cleaning up existing tables...");
    await cleanupLocalTables();
    
    console.log("üèóÔ∏è  Creating tables...");
    await createLocalTables();
    
    console.log("‚úÖ Database setup complete!");
  } catch (error) {
    console.error({ error }, "Failed to setup database");
    throw error;
  }
}

export async function copyDB(): Promise<void> {
  try {
    console.log("üì• Copying production data to local tables...");
    await copyProductionData();
    
    console.log("‚úÖ Database setup complete!");
  } catch (error) {
    console.error({ error }, "Failed to setup database");
    throw error;
  }
}

export async function cleanupLocalDB(): Promise<void> {
  try {
    console.log("üõë Stopping DynamoDB Docker container...");
    
    // Stop and remove the container
    await runCommand("docker", ["stop", "dynamodb-local"]);
    await runCommand("docker", ["rm", "dynamodb-local"]);
    
    console.log("‚úÖ Database cleanup complete!");
  } catch (error) {
    console.error({ error }, "Failed to cleanup database");
  }
}

async function cleanupLocalTables(): Promise<void> {
  const localClient = new DynamoDBClient({
    region: "local",
    endpoint: LOCAL_ENDPOINT
  });

  try {
    // List existing tables
    const listCommand = new ListTablesCommand({});
    const existingTables = await localClient.send(listCommand);
    
    if (!existingTables.TableNames || existingTables.TableNames.length === 0) {
      console.log("‚ÑπÔ∏è No existing tables found");
      return;
    }

    console.log(`üìã Found ${existingTables.TableNames.length} existing tables`);

    // Delete tables if they exist
    const tablesToDelete = [
      process.env.NODE_OPERATORS_TABLE_NAME,
      process.env.VALIDATORS_TABLE_NAME,
      process.env.NETWORK_STATS_TABLE_NAME
    ];
    
    for (const tableName of tablesToDelete) {
      if (existingTables.TableNames.includes(tableName || "")) {
        console.log(`üóëÔ∏è  Deleting table: ${tableName}`);
        try {
          await localClient.send(new DeleteTableCommand({ TableName: tableName }));
          console.log(`‚úÖ Deleted table: ${tableName}`);
          
          // Wait for table to be deleted
          await new Promise(resolve => setTimeout(resolve, 2000));
        } catch (error: any) {
          if (error.name === "ResourceNotFoundException") {
            console.log(`‚ÑπÔ∏è Table ${tableName} does not exist`);
          } else {
            console.warn(`‚ö†Ô∏è Failed to delete table ${tableName}:`, error.message);
          }
        }
      } else {
        console.log(`‚ÑπÔ∏è Table ${tableName} does not exist`);
      }
    }
  } catch (error) {
    console.error({ error }, "Failed to cleanup local tables");
    throw error;
  }
}

async function createLocalTables(): Promise<void> {
  const localClient = new DynamoDBClient({
    region: "local",
    endpoint: LOCAL_ENDPOINT
  });

  // Create OLD node operators table (discordId primary key) - to receive production data
  console.log(`üèóÔ∏è  Creating OLD table: ${process.env.NODE_OPERATORS_TABLE_NAME}`);
  try {
    await localClient.send(new CreateTableCommand({
      TableName: process.env.NODE_OPERATORS_TABLE_NAME,
      AttributeDefinitions: [
        {
          AttributeName: "discordId",
          AttributeType: "S",
        },
        {
          AttributeName: "address",
          AttributeType: "S",
        },
      ],
      KeySchema: [
        {
          AttributeName: "discordId",
          KeyType: "HASH", // Partition key - OLD structure with discordId as primary key
        },
      ],
      GlobalSecondaryIndexes: [
        {
          IndexName: "AddressIndex",
          KeySchema: [
            {
              AttributeName: "address",
              KeyType: "HASH",
            },
          ],
          Projection: {
            ProjectionType: "ALL",
          },
          ProvisionedThroughput: {
            ReadCapacityUnits: 5,
            WriteCapacityUnits: 5,
          },
        },
      ],
      ProvisionedThroughput: {
        ReadCapacityUnits: 5,
        WriteCapacityUnits: 5,
      },
    }));
    console.log(`‚úÖ Created OLD table: ${process.env.NODE_OPERATORS_TABLE_NAME}`);
  } catch (error: any) {
    if (error.name === "ResourceInUseException") {
      console.log(`‚ÑπÔ∏è Table ${process.env.NODE_OPERATORS_TABLE_NAME} already exists`);
    } else {
      console.error({ error }, `Failed to create table: ${process.env.NODE_OPERATORS_TABLE_NAME}`);
      throw error;
    }
  }

  // Create validators table
  console.log(`üèóÔ∏è  Creating table: ${process.env.VALIDATORS_TABLE_NAME}`);
  try {
    await localClient.send(new CreateTableCommand({
      TableName: process.env.VALIDATORS_TABLE_NAME,
      AttributeDefinitions: [
        {
          AttributeName: "validatorAddress",
          AttributeType: "S",
        },
        {
          AttributeName: "nodeOperatorId",
          AttributeType: "S",
        },
      ],
      KeySchema: [
        {
          AttributeName: "validatorAddress",
          KeyType: "HASH", // Partition key
        },
      ],
      GlobalSecondaryIndexes: [
        {
          IndexName: "NodeOperatorIndex",
          KeySchema: [
            {
              AttributeName: "nodeOperatorId",
              KeyType: "HASH",
            },
          ],
          Projection: {
            ProjectionType: "ALL",
          },
          ProvisionedThroughput: {
            ReadCapacityUnits: 5,
            WriteCapacityUnits: 5,
          },
        },
      ],
      ProvisionedThroughput: {
        ReadCapacityUnits: 5,
        WriteCapacityUnits: 5,
      },
    }));
    console.log(`‚úÖ Created table: ${process.env.VALIDATORS_TABLE_NAME}`);
  } catch (error: any) {
    if (error.name === "ResourceInUseException") {
      console.log(`‚ÑπÔ∏è Table ${process.env.VALIDATORS_TABLE_NAME} already exists`);
    } else {
      console.error({ error }, `Failed to create table: ${process.env.VALIDATORS_TABLE_NAME}`);
      throw error;
    }
  }

  // Create network stats table
  console.log(`üèóÔ∏è  Creating table: ${process.env.NETWORK_STATS_TABLE_NAME}`);
  try {
    await localClient.send(new CreateTableCommand({
      TableName: process.env.NETWORK_STATS_TABLE_NAME,
      AttributeDefinitions: [
        {
          AttributeName: "epochNumber",
          AttributeType: "N",
        },
        {
          AttributeName: "timestamp",
          AttributeType: "N",
        },
      ],
      KeySchema: [
        {
          AttributeName: "epochNumber",
          KeyType: "HASH", // Partition key
        },
      ],
      GlobalSecondaryIndexes: [
        {
          IndexName: "TimestampIndex",
          KeySchema: [
            {
              AttributeName: "timestamp",
              KeyType: "HASH",
            },
          ],
          Projection: {
            ProjectionType: "ALL",
          },
          ProvisionedThroughput: {
            ReadCapacityUnits: 5,
            WriteCapacityUnits: 5,
          },
        },
      ],
      ProvisionedThroughput: {
        ReadCapacityUnits: 5,
        WriteCapacityUnits: 5,
      },
    }));
    console.log(`‚úÖ Created table: ${process.env.NETWORK_STATS_TABLE_NAME}`);
  } catch (error: any) {
    if (error.name === "ResourceInUseException") {
      console.log(`‚ÑπÔ∏è Table ${process.env.NETWORK_STATS_TABLE_NAME} already exists`);
    } else {
      console.error({ error }, `Failed to create table: ${process.env.NETWORK_STATS_TABLE_NAME}`);
      throw error;
    }
  }

  // Wait for tables to be active
  console.log("‚è≥ Waiting for tables to become active...");
  await new Promise(resolve => setTimeout(resolve, 5000));
}

async function copyProductionData(): Promise<void> {
  // Production client
  const prodClient = DynamoDBDocumentClient.from(new DynamoDBClient({
    region: PRODUCTION_REGION
  }));

  // Local client  
  const localClient = DynamoDBDocumentClient.from(new DynamoDBClient({
    region: "local",
    endpoint: LOCAL_ENDPOINT
  }));

  // Copy validators data with pagination
  try {
    console.log(`üì• Copying data from ${process.env.PROD_VALIDATORS_TABLE_NAME}...`);
    let lastEvaluatedKey: Record<string, any> | undefined = undefined;
    let totalValidators = 0;
    let pageNum = 0;

    do {
      pageNum++;
      const validatorsData = await prodClient.send(new ScanCommand({
        TableName: process.env.PROD_VALIDATORS_TABLE_NAME,
        ExclusiveStartKey: lastEvaluatedKey,
        Limit: 100
      }));

      if (validatorsData.Items && validatorsData.Items.length > 0) {
        for (const item of validatorsData.Items) {
          await localClient.send(new PutCommand({
            TableName: process.env.VALIDATORS_TABLE_NAME,
            Item: item
          }));
        }
        totalValidators += validatorsData.Items.length;
        console.log(`üìÑ Validators page ${pageNum}: Copied ${validatorsData.Items.length} items. Total so far: ${totalValidators}`);
      }

      lastEvaluatedKey = validatorsData.LastEvaluatedKey;
    } while (lastEvaluatedKey);

    console.log(`‚úÖ Copied ${totalValidators} validators total`);
  } catch (error) {
    console.warn("‚ö†Ô∏è Could not copy validators data:", error);
  }

  // Copy node operators data with pagination
  try {
    console.log(`üì• Copying data from ${process.env.PROD_NODE_OPERATORS_TABLE_NAME} ‚Üí ${process.env.NODE_OPERATORS_TABLE_NAME} (OLD table)...`);
    let lastEvaluatedKey: Record<string, any> | undefined = undefined;
    let totalOperators = 0;
    let pageNum = 0;

    do {
      pageNum++;
      const operatorsData = await prodClient.send(new ScanCommand({
        TableName: process.env.PROD_NODE_OPERATORS_TABLE_NAME,
        ExclusiveStartKey: lastEvaluatedKey,
        Limit: 10
      }));

      if (operatorsData.Items && operatorsData.Items.length > 0) {
        for (const item of operatorsData.Items) {
          await localClient.send(new PutCommand({
            TableName: process.env.NODE_OPERATORS_TABLE_NAME,
            Item: item
          }));
        }
        totalOperators += operatorsData.Items.length;
        console.log(`üìÑ Operators page ${pageNum}: Copied ${operatorsData.Items.length} items. Total so far: ${totalOperators}`);
      }

      lastEvaluatedKey = operatorsData.LastEvaluatedKey;
    } while (lastEvaluatedKey);

    console.log(`‚úÖ Copied ${totalOperators} node operators to OLD table`);
    console.log(`üìã Migration ready: OLD table (${process.env.NODE_OPERATORS_TABLE_NAME}) ‚Üí NEW table (${process.env.NODE_OPERATORS_TABLE_NAME})`);
  } catch (error) {
    console.warn("‚ö†Ô∏è Could not copy node operators data:", error);
  }

  // Copy network stats data with pagination
  try {
    console.log(`üì• Copying data from ${process.env.PROD_NETWORK_STATS_TABLE_NAME}...`);
    let lastEvaluatedKey: Record<string, any> | undefined = undefined;
    let totalStats = 0;
    let pageNum = 0;

    do {
      pageNum++;
      const networkStatsData = await prodClient.send(new ScanCommand({
        TableName: process.env.PROD_NETWORK_STATS_TABLE_NAME,
        ExclusiveStartKey: lastEvaluatedKey
      }));

      if (networkStatsData.Items && networkStatsData.Items.length > 0) {
        for (const item of networkStatsData.Items) {
          await localClient.send(new PutCommand({
            TableName: process.env.NETWORK_STATS_TABLE_NAME,
            Item: item
          }));
        }
        totalStats += networkStatsData.Items.length;
        console.log(`üìÑ Network stats page ${pageNum}: Copied ${networkStatsData.Items.length} items. Total so far: ${totalStats}`);
      }

      lastEvaluatedKey = networkStatsData.LastEvaluatedKey;
    } while (lastEvaluatedKey);

    console.log(`‚úÖ Copied ${totalStats} network stats total`);
  } catch (error) {
    console.warn("‚ö†Ô∏è Could not copy network stats data:", error);
  }
}

async function runCommand(command: string, args: string[]): Promise<void> {
  return new Promise((resolve, reject) => {
    const process = spawn(command, args, {
      stdio: ["pipe", "pipe", "pipe"]
    });

    process.on("close", (code) => {
      if (code === 0) {
        resolve();
      } else {
        reject(new Error(`Command failed with code ${code}`));
      }
    });

    process.on("error", (error) => {
      reject(error);
    });
  });
} 

(async () => {
    console.log("üìä Setting up local database for migration testing...");
    console.log("üîÑ This will create:");
    console.log(`   ‚Ä¢ OLD table: ${process.env.NODE_OPERATORS_TABLE_NAME || 'sparta-node-operators-dev'} (discordId primary key)`);
    console.log(`   ‚Ä¢ NEW table: ${process.env.NODE_OPERATORS_TABLE_NAME || 'sparta-node-op-dev'} (address primary key)`);
    console.log("üì• Production data will be copied to the OLD table");
    console.log("üß™ Then you can test migration: OLD ‚Üí NEW table\n");
    
    await setupLocalDB();
    await copyDB();
    
    console.log("\n‚úÖ Setup complete! Ready for migration testing");
    console.log("üìù Next steps:");
    console.log("   1. Ensure your .env file has these variables:");
    console.log("      NODE_OPERATORS_TABLE_NAME=sparta-node-operators-dev");
    console.log("      NODE_OPERATORS_TABLE_NAME=sparta-node-op-dev");  
    console.log("      LOCAL_DYNAMO_DB=true");
    console.log("      DYNAMODB_ENDPOINT=http://localhost:8000");
    console.log("      BACKEND_API_KEY=your-api-key");
    console.log("   2. Run migration: bun run migration:test");
    console.log("   3. Restart application to use new table");
})();